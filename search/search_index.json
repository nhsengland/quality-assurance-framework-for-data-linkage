{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"evaluation/","title":"Evaluation","text":""},{"location":"evaluation/#verification-and-validity","title":"Verification and Validity","text":"<p>Verification ensures that the project is built correctly according to specifications, validation ensures that the project addresses the right problem and delivers the expected results.</p> <p>Verification and Validity</p> RecommendationsRAG <p>Verification in a data linkage project involves:</p> <ul> <li> <p>Ensuring alignment of methods and processes with project specifications</p> </li> <li> <p>Conducting code reviews and unit testing</p> </li> <li> <p>Confirming the robustness of the chosen methodology through expert review</p> </li> <li> <p>Clearly documenting any deviations from requirements</p> </li> </ul> <p>Validation in a data linkage project entails:</p> <ul> <li>Sense checking, comparing results against established truths or manual inspections to verify plausibility with users and data linkage experts.</li> </ul> <p>\ud83d\udd34 RED:  Insufficient practices in verification, validity checks. No established routine for sense checking or peer reviews in data linkage projects.</p> <p>\ud83d\udfe1 AMBER:  Intermittent verification and validity checks with some inconsistencies, along with occasional sense checking and peer reviews that may lack consistency.</p> <p>\ud83d\udfe2 GREEN:  Verification and validity checks performed. Clear practices and established routines for sense checking and thorough peer reviews.</p>"},{"location":"evaluation/#quality-of-data-linkage","title":"Quality of data linkage","text":"<p>Effective evaluation of data linkage quality builds trust in the results and provides a quantitative assessment of the linkage process's impact on subsequent procedures, such as statistics reliant on the linked data.</p> <p>Quality of data linkage</p> RecommendationsRAG <p>Record-level checks</p> <ul> <li> <p>Identifying errors in specific records</p> </li> <li> <p>Analysing metadata like matching steps and confidence scores</p> </li> </ul> <p>Aggregate-level checks</p> <ul> <li> <p>Identifying agreement patterns in linking variables</p> </li> <li> <p>Identifying biases</p> </li> <li> <p>Assessing analytical outcomes' sensitivity to parameter adjustments (if data permits)</p> </li> <li> <p>Cross-referencing outcomes with alternative sources or linkage methods</p> </li> </ul> <p>Other checks</p> <ul> <li> <p>Utilising clerical resources (if available) for structured reviews to establish a ground truth</p> </li> <li> <p>Computing precision, recall, and F1-score (if a ground truth is established)</p> </li> </ul> <p>\ud83d\udd34 RED:  The results of the Data Linkage process have not been evaluated. The impact of the linked data quality is unknown on subsequent procedures.</p> <p>\ud83d\udfe1 AMBER:  Partial evaluation of the linked data is available but it inconsistent.</p> <p>\ud83d\udfe2 GREEN:  A process to evaluate the data linkage quality is in place and is comprehensive. Users of linked data are aware of the outcomes of this evaluation.</p>"},{"location":"evaluation/#speed","title":"Speed","text":"<p>Efficient data linkage balances timeliness with accurate results.</p> <p>Speed</p> RecommendationsRAG <ul> <li> <p>Track processing speed and identify bottlenecks for improvement</p> </li> <li> <p>Tackle factors like dataset size, hardware limitations, and inefficient algorithms</p> </li> <li> <p>Ensure your process can handle larger datasets and increased frequency without sacrificing quality</p> </li> </ul> <p>\ud83d\udd34 RED:  The linkage process speed has not been assessed. No attempts to improve speed have been made.</p> <p>\ud83d\udfe1 AMBER:  Some efforts are made to improve the efficiency of the linkage but partially or not implemented.</p> <p>\ud83d\udfe2 GREEN:  Efficient data linkage processes have been achieved through proactive reviews and enhancements.</p>"},{"location":"evaluation/#computational-resources-used","title":"Computational Resources Used","text":"<p>Computational resources used refers to the computing infrastructure required to support the Data Linkage process.</p> <p>Computational resources used</p> RecommendationsRAGWorked examples <ul> <li> <p>The size of the datasets being linked influences the computational resources - as larger data sets require more processing power and time, memory, and storage.</p> </li> <li> <p>Allocate sufficient computational resources for diverse types of DL projects and optimise processing times to ensure efficient data linkage.</p> </li> </ul> <p>\ud83d\udd34 RED:  Insufficient computational resources which forces the use of suboptimal linkage methods.</p> <p>\ud83d\udfe1 AMBER:  There is allocation of resources, but it is not ideal for complex data linkage projects.</p> <p>\ud83d\udfe2 GREEN:  Adequate allocation of computational resources for different types of DL projects to minimise the linkage error.</p> Function to measure the run time of a function <p><pre><code>import timeit\nimport time\n\ndef measure_function_time(f):\n'''\n    Times how long a function takes to run in seconds\n    '''\n\n    start_time = timeit.default_timer()\n    f()\n    end_time = timeit.default_timer()\n    print(f\"It took {end_time-start_time}s to call the function\")\n\n\ndef wait_15_seconds():\n'''\n    Function called to wait 15 seconds before executing print statement\n    '''\n    time.sleep(15)\n    print('Waited 15 seconds!')\n\nmeasure_function_time(wait_15_seconds)\n</code></pre> <pre><code>Waited 15 seconds!\nIt took 15.000131200999988s to call the function\n</code></pre> </p> <p>Understanding how long a function takes to run can be used to understand resource allocation in your machine, and can be used as a proxy to understand the load in different parts of the data linkage process.</p> Function to measure CPU usage <p><pre><code>import psutil\n\ndef get_cpu_usage(time_period):\n'''\n    Prints the CPU usage over a time period\n    '''\n\n    cpu_usage = psutil.cpu_percent(time_period)\n    return cpu_usage\n\ntime_period = 10\n\nprint('The CPU usage for the last',time_period,'seconds is : ',get_cpu_usage(time_period))\n</code></pre> <pre><code>The CPU usage for the last 10 seconds is :  0.4\n</code></pre> Understanding how CPU is being used helps understand resource allocation in your machine, and can be used as a proxy to understand the load of different data linkage models </p> Function to measure the ram memory being used <p><pre><code>import psutil\n\ndef get_ram_memory():\n'''\n    Returns the % of RAM being used, and number of GB being used by RAM at a moment in time\n    '''\n\n    ram_percentage_used = psutil.virtual_memory()[2]\n    ram_gb_used = psutil.virtual_memory()[3]/1000000000\n    return ram_percentage_used, ram_gb_used\n\n\nprint(get_ram_memory())\n</code></pre> <pre><code>(6.9, 0.774934528)\n</code></pre> Understanding how RAM is being used helps understand resource allocation in your machine, and can be used as a proxy to understand the load of different data linkage models </p> <p>If you have any ideas or feedback you'd like to give the team, feel free to contact us</p>"},{"location":"glossary/","title":"Glossary","text":"Term Definition Bias analysis Bias analysis is concerned with identifying and mitigating biases that may be present in algorithms and how these can affect the outcomes of a model. Blocking rules A blocking rule is a criterion or set of criteria used to group records into sub-sets or \u2018blocks\u2019 before the actual matching or linkage process takes place. The purpose of blocking is to limit the number of record pairs that need to be compared during the matching process, making the linkage more efficient and reducing the computational resources required Cardinality The cardinality of a set is a measure of the number of unique elements of the set. Clerical reviews Clerical reviews, in the context of data linkage, refer to manual inspections or examinations conducted by human operators or clerks to verify and validate the accuracy of the matched record. Confidence score Record-level information that consists in a number (for example, between 0 and 1, or 0 and 100%) that represents a confidence that the output of the matching algorithm is correct for a given record.This is different from accuracy, which defines the percentage of correct predictions made from all predictions. Confidence scores can be formulated in different ways for different algorithms. It may not be possible to interpret a confidence score as a direct measure of the probability of a record being correct. But generally speaking, a higher confidence score should correlate with a higher likelihood of the record having been correctly matched. Data linkage Data linkage is the process of bringing together records that pertain to the same entity, such as a person, place, or organisation --OR-- A process which brings together two or more sets of administrative or survey data to produce information (linked data) that can be used for research or statistical purposes. Data matching Data matching is often used interchangeably with data linkage. However, while data linkage refers to the entire process of bringing together records from different data sets, data matching refers to the specific approach used to recognise that such records refer to the same entity. Deterministic matching Deterministic matching uses deterministic rules to decide whether a pair of records match or not. The match does not need to be perfect. If the rules establish that partial-matches are accepted, then the algorithm can produce a non-perfect match. Identifiable An identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. Joining Joining refers to the process of using unique keys that identify the same records in different data sets. Linked asset/ Linked data The product of bringing together data from multiple sources. Non-identifiable The opposite of identifiable Precision It is the ratio of the true positive matches to the total number of records identified as matches. Probabilistic matching Probabilistic matching involves the calculation of conditional probabilities to determine the likelihood that a record pair is a match. Pseudonymisation The processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organizational measures to ensure that the personal data are not attributed to an identified or identifiable natural person. Recall It is the ratio of true positive matches to the total number of true matches. Sensitivity Analysis Sensitivity analysis in data linkage involves systematically varying the parameters or methods used in the linkage process to evaluate how sensitive the study results are to these changes. It helps researchers assess the robustness of their findings and identify potential sources of bias or error."},{"location":"implementation/","title":"Implementation","text":""},{"location":"implementation/#techniques-and-tools","title":"Techniques and Tools","text":"<p>The selection of tools and techniques for Data Linkage must be documented and justified to suit the specific application.</p> <p>Type: Process</p> RecommendationsRAG <p>Some examples of options are</p> <ul> <li> <p>Matching methods: deterministic, probabilistic, hybrid or machine learning-based matching.</p> </li> <li> <p>Tools: publicly available libraries (e.g. Splink), internal off-the shelf services (e.g. MPS), third-party proprietary software (AWS Glue).</p> </li> </ul> <p>The selection of tools and techniques is influenced by various factors. Employ the Triage guidelines to ensure comprehensive consideration of all relevant factors when choosing the appropriate tools and techniques.</p> <p>When employing an existing model, justify this decision and clearly specify the version being used. Explain why it aligns with your case and data requirements.</p> <p>\ud83d\udd34 RED:  Inappropriate or inconsistent use of linkage techniques that are not aligned with the nature and quality of the data, resulting in inaccurate or unreliable linkages. There is a reliance on methodologies and tools that are unsuitable or not fit for purpose, leading to suboptimal results.</p> <p>\ud83d\udfe1 AMBER:  Some consideration given to the nature and quality of the data when selecting linkage techniques, but there may still be instances where inappropriate methodologies and tools are used, leading to room for improvement in the applicability or efficiency of the linkages.</p> <p>\ud83d\udfe2 GREEN:  Careful consideration of the nature and quality of the data to select appropriate linkage techniques. State-of-the-art methodologies and tools are employed, including deterministic matching for exact matches and probabilistic matching for likely matches. There is a focus on the efficient and accurate delivery of results, without relying on unsuitable methodologies or tools.</p>"},{"location":"implementation/#configuration-of-linkage-parameterssettings","title":"Configuration of linkage parameters/settings","text":"<p>Data Linkage models depend on parameters such as: the choice of blocking variables, matching thresholds, weight assignments for different linkage features, etc.</p> <p>The quality assurance of linkage parameters involves different requirements if the data linker sets up a bespoke model (A) or use an established model (B).</p> <p>Type: Data</p> RecommendationsRAG <p>Option (A) - Bespoke model</p> <ul> <li> <p>Fine-tune parameters: Adjust settings like matching thresholds and weights based on your data quality and business case.</p> </li> <li> <p>Data-driven decisions: Test different configurations to optimise accuracy and reliability, and document final choices for transparency.</p> </li> <li> <p>Version control: Share and store parameter settings for future reference.</p> </li> </ul> <p>Option (B) - Established model</p> <ul> <li> <p>Understand your parameters: Evaluate what is built-in to the model and their impact on results.</p> </li> <li> <p>Documentation: Rely on existing model documentation to assess settings.</p> </li> <li> <p>Investigate data interaction: Test and document how the model's parameters interact with your specific data.</p> </li> </ul> <p>Employ the Triage guidelines to ensure comprehensive consideration of all relevant factors when choosing the appropriate parameters and settings.</p> <p>Option (A) - Bespoke model</p> <p>\ud83d\udd34 RED:  Linkage parameters are inadequately configured leading to inaccurate or unreliable linkages. No consideration for data quality or business needs.</p> <p>\ud83d\udfe1 AMBER:  Some parameter consideration, but inconsistent. Iteration lacks systematic approach.</p> <p>\ud83d\udfe2 GREEN:  Linkage parameters are carefully configured based on data quality and specific business needs. Adjustment is made when necessary to enhance accuracy and reliability. Iterative refinement is systematic, informed by evaluation and quality assessment.</p> <p>Option (B) - Established model</p> <p>\ud83d\udd34 RED:  Data linker is unaware of the parameters involved in the established model, and has not carried out an impact assessment on the data in hand.</p> <p>\ud83d\udfe1 AMBER:  Data linker is aware of the parameters involved in the established model, but has not carried out an impact assessment on the data in hand.</p> <p>\ud83d\udfe2 GREEN:  Data linker is aware of the parameters involved in the established model, and has carried out an impact assessment on the data in hand.</p>"},{"location":"implementation/#version-control","title":"Version control","text":"<p>Version control guarantees consistent, traceable, and reproducible outcomes for the data linkage process.</p> <p>Type: Process</p> RecommendationsRAG <ul> <li>Code: Track changes to the main linkage process code for easy debugging and updates.</li> <li>Libraries: Monitor any external code used.</li> <li>Model Parameters: Record adjustments to settings for transparency and optimisation.</li> <li>Analyses: Preserve profiling and test outcomes for future comparison, enhancement, and methodology validation.</li> <li>Assessment Card:  Monitor the evolution of your linkage methodology and its performance.</li> </ul> <p>\ud83d\udd34 RED:  Absence of a version control process in the data linkage procedure.</p> <p>\ud83d\udfe1 AMBER:  Basic version control process is in place but lacks comprehensive tracking and management features.</p> <p>\ud83d\udfe2 GREEN:  Robust version control system is in place, effectively tracking changes and managing different versions throughout the data linkage process.</p> <p>If you have any ideas or feedback you'd like to give the team, feel free to contact us</p>"},{"location":"overall-considerations/","title":"Overall considerations","text":""},{"location":"overall-considerations/#uncertainty-management","title":"Uncertainty management","text":"<p>Addressing uncertainty in the Data Linkage process enhances confidence in its outcomes.</p> <p>Transparent communication of risks, assumptions, and dependencies is essential, alongside periodic reassessment of these factors.</p> <p>Uncertainty management</p> RecommendationsRAG <p>Three key aspects require consideration:</p> <ul> <li> <p>Risks: Risks pertain to potential events that may adversely affect project objectives, outcomes, or schedules. Managing risks entails identifying, evaluating, prioritising, and mitigating them to safeguard project success.</p> </li> <li> <p>Assumptions: These are accepted beliefs on which the Data Linkage model relies. Managing assumptions involves acknowledging, documenting, and validating these beliefs.</p> </li> <li> <p>Dependencies: Interdependencies among project components may exist. Managing dependencies entails identifying and monitoring these relationships and proactively addressing them during the planning phase.</p> </li> </ul> <p>\ud83d\udd34 RED:  Uncertainties are not recognised, managed or communicated.</p> <p>\ud83d\udfe1 AMBER:  Some uncertainties are recognised, but the communication and management process could be improved.</p> <p>\ud83d\udfe2 GREEN:  Uncertainties are comprehensively recognised, managed and communicated.</p>"},{"location":"overall-considerations/#communication-of-changes","title":"Communication of changes","text":"<p>Communication of Changes involves clearly and effectively sharing updates or modifications to core data linkage algorithms or methodologies.</p> <p>Communication of changes</p> RecommendationsRAG <ul> <li> <p>Establish a roadmap for updates.</p> </li> <li> <p>Inform all stakeholders of planned changes to prevent disruption.</p> </li> </ul> <p>\ud83d\udd34 RED:  Absence of a roadmap for changes and ineffective engagement with stakeholders lead to changes being introduced without proper notification.</p> <p>\ud83d\udfe1 AMBER:  Partial roadmap for changes and non-timely engagement with stakeholders lead to changes being introduced poor notification.</p> <p>\ud83d\udfe2 GREEN:  Presence of a roadmap for changes and effective engagement with stakeholders ensure changes are introduced with proper notification.</p>"},{"location":"overall-considerations/#safety","title":"Safety","text":"<p>Data Linkage must occur in a safe environments and be a regulated process.</p> <p>Safety</p> RecommendationsRAG <ul> <li> <p>Regulations: Adhere to data protection regulations like GDPR and the Data Protection Act 2018. Supporting documents like Data Protection Impact Assessments (DPIAs) and Data Sharing Agreements (DSAs) should demonstrate compliance.</p> </li> <li> <p>Data minimisation: Separate personal identifiable information and clinical outcomes and ensure linked records don't reveal sensitive information beyond what's been agreed upon. Consider using Privacy Enhancing Techniques (PETs) to protect individual privacy (for example, encryption)</p> </li> </ul> <p>\ud83d\udd34 RED:  Data regulations and data minimisation principles are not followed. Unwanted disclosive information might be released.</p> <p>\ud83d\udfe1 AMBER:  Partial adherence to data regulations and data minimisation principles. Some risk of disclosive information been released exists.</p> <p>\ud83d\udfe2 GREEN:  Data regulations and data minimisation principles are followed comprehensively.</p>"},{"location":"overall-considerations/#ethics-and-fairness","title":"Ethics and fairness","text":"<p>Ethical principles should guide every stage of the Data Linkage process.</p> <p>Ethics and fairness</p> RecommendationsRAG <ul> <li> <p>Fairness: Ensuring that the project and its outcomes are non-discriminatory. Identifying issues such as bias or uneven error distribution among different population segments.</p> </li> <li> <p>Accountability: Ensuring that individuals in the Data Linkage workflow are accountable for the decisions made at each stage.</p> </li> <li> <p>Sustainability: Ensuring that the Data Linkage model remains relevant as real-world data evolve over time.</p> </li> <li> <p>Transparency: Offering clear access to processes and decisions.</p> </li> </ul> <p>\ud83d\udd34 RED:  No ethical considerations are made. Possible risks of bias or misuse of data are unaddressed.</p> <p>\ud83d\udfe1 AMBER:  Some ethical considerations, but without thorough implementation or follow-through. Risks of hidden bias or misuse of data may still exist.</p> <p>\ud83d\udfe2 GREEN:  Comprehensive ethical considerations. Risks are understood and disclosed.</p>"},{"location":"overall-considerations/#information-governance","title":"Information governance","text":"<p>Data linkage projects need clear structure and accountability. Defining roles, assigning ownership, and implementing strong oversight lets organisations manage complex Data Linkage projects while ensuring a high standard of information governance.</p> <p>Information governance</p> RecommendationsRAG <ul> <li> <p>Roles: Clearly define and document key roles like Information Asset Owner (IAO) and Data Linkage Owner (DLO).</p> </li> <li> <p>Responsibilities: DLO is responsible for ensuring projects align with Quality Assurance Framework principles.</p> </li> </ul> <p>\ud83d\udd34 RED:  Lack of clear roles and responsibilities. THe principles set out in the Quality Assurance Framework are not followed.</p> <p>\ud83d\udfe1 AMBER:  Roles and responsibilities not clearly defined. Some awareness and adherence to Quality Assurance Framework principles, but gaps or inconsistencies in implementation.</p> <p>\ud83d\udfe2 GREEN:  Well-established roles and responsibilities. The Quality Assurance Framework principles are followed and there are regular reviews and updates.</p>"},{"location":"overall-considerations/#community-engagement","title":"Community engagement","text":"<p>Community Engagement in a Data Linkage project involves active collaboration and discussion with other leaders and experts within the field.</p> <p>Community engagement</p> RecommendationsRAG <ul> <li> <p>Collaborate to innovate: Join the Data Linkage conversation for knowledge exchange, technique comparison, and process optimisation. Collective wisdom drives the field forward.</p> </li> <li> <p>Contribute, elevate: Share your tools, code, and experiences to empower the Data Linkage community.</p> </li> </ul> <p>\ud83d\udd34 RED:  No engagement with the broader data linkage community.</p> <p>\ud83d\udfe1 AMBER:  Occasional engagement with the community but lacks systematic involvement.</p> <p>\ud83d\udfe2 GREEN:  Regular engagement with the data linkage community, promoting knowledge exchange and collaboration.</p>"},{"location":"overall-considerations/#knowledge-management-documentation-and-transparency","title":"Knowledge Management (Documentation and transparency)","text":"<p>Documentation and Transparency refer to the practices of recording and sharing the methodologies, processes, and outcomes of the Data Linkage project in a way that can be easily understood by others.</p> <p>Knowledge Management (Documentation and transparency)</p> RecommendationsRAG <ul> <li> <p>Document decision making: refer to Transparency and Accountability in Ethics &amp; Fairness</p> </li> <li> <p>Disseminate knowledge: Make your documentation user-friendly and readily available to all stakeholders. Code goes open-source if possible.</p> </li> </ul> <p>\ud83d\udd34 RED:  Lack of transparency and inadequate or inaccessible documentation of the project, making it difficult for others to understand and replicate. Code is not available.</p> <p>\ud83d\udfe1 AMBER:  Partially documented and accessible only by few. Code is internally available.</p> <p>\ud83d\udfe2 GREEN:  Transparent documentation that can be easily understood and accessed by others. Code is fully published and available on github.</p>"},{"location":"overall-considerations/#continuous-improvement-and-maintenance","title":"Continuous improvement and maintenance","text":"<p>Continuous Improvement entails the systematic documentation of lessons learnt throughout the course of a linkage project, encompassing aspects such as methodology, data, and tools. These insights are then consolidated into a backlog of improvement suggestions, complete with corresponding resource requirements, to be assessed in subsequent project iterations.</p> <p>Continuous improvement and maintenance</p> RecommendationsRAG <ul> <li> <p>Frequency: Determining the frequency of reassessment, particularly for ongoing projects, is crucial to ensure the ongoing relevance of quality assurance procedures, especially in the face of potential alterations in data and requirements.</p> </li> <li> <p>Feedback: Provide feedback to the data providers regarding data quality matters, and capture feedback from the users of the linked data to enhance data linkage process.</p> </li> <li> <p>Backlog: Document lessons learned \u2013 methods, data, tools \u2013 and turn them into a backlog of actionable improvements with resource estimates.</p> </li> <li> <p>Adapt and evolve: Regularly review your quality assurance plan, especially for ongoing projects, to stay relevant as data and needs change.</p> </li> </ul> <p>\ud83d\udd34 RED:  No process is in place to capture and relay feedback. No actionable insights obtained to improve the Data Linkage.</p> <p>\ud83d\udfe1 AMBER:  Some feedback is captured but not systematically. Some of the insights are used to improve the Data Linkage.</p> <p>\ud83d\udfe2 GREEN:  A robust process is in place to capture and relay feedback. The insights are used to improve the Data Linkage.</p> <p>If you have any ideas or feedback you'd like to give the team, feel free to contact us</p>"},{"location":"preparation/","title":"Data preparation","text":""},{"location":"preparation/#profiling","title":"Profiling","text":"<p>Data profiling is the process of examining the data and collecting statistics and information about that data. This is important for selecting the right blocking and matching variables and ensure accurate linkage results.</p> <p>Data Profiling</p> RecommendationsRAGWorked examples <p>This includes establishing:</p> <ul> <li>Data dictionary (data types, data meaning, etc)</li> <li>Completeness (presence of missing or null values)</li> <li>Uniqueness / cardinality (how many and which unique values a variable can have)</li> <li>Validity of variable values (the values are included in the data dictionary, are valid, have a meaning)</li> <li>Distribution of values / outliers</li> <li>Basic statistics (min, max, mean, variance)</li> <li>Correlations and functional dependencies between variables</li> </ul> <p>\ud83d\udd34 RED:  No data profiling is performed before linking, leading to potential inconsistencies in the linked data.</p> <p>\ud83d\udfe1 AMBER:  Some data profiling is performed before linking, but it could be more thorough and systematic.</p> <p>\ud83d\udfe2 GREEN:  Comprehensive data profiling is performed before linking, identifying and resolving inconsistencies in datasets to be linked.</p> Data Dictionary Creation <p><pre><code>import pandas as pd\n\ninput_data = [[1, 20240224, 'sample_name']]\ninput_schema = ['id', 'dob', 'name']\ninput_df = pd.DataFrame(input_data, columns = input_schema)\n\ndef get_data_types(df):\n'''\n    function to get data types of columns in a dataframe\n\n    parameters:\n        df: Input DataFrame\n\n    returns:\n        DataFrame: DataFrame with columns 'Column' and 'Data Type'\n    '''\n\n    data_types = [[col_name, str(col_type)] for col_name, col_type in zip(df.columns, df.dtypes)]\n\n    schema =  ['Column', 'Data Type']\n\n    result_df = pd.DataFrame(data_types, columns = schema)\n\n    return result_df\n\noutput = get_data_types(input_df)\n\nprint(output)\n</code></pre> <pre><code>  Column Data Type\n0     id     int64\n1    dob     int64\n2   name    object\n</code></pre> </p> Completeness <p><pre><code>import pandas as pd\n\ninput_data = [[1, 20240224, 'sample_name'], [None, 20240224, 'sample_name'],[1, 20240224, None], [1, 20240224, None]]\ninput_schema = ['id', 'dob', 'name']\ninput_df = pd.DataFrame(input_data, columns = input_schema)\n\ndef null_percentage(df):\n'''\n    Function to calculate the percentage of null values in each column of a pandas DataFrame.\n\n    Parameters:\n        df (DataFrame): Input DataFrame\n\n    Returns:\n        DataFrame: DataFrame with columns 'Column' and 'Null Percentage'\n    '''\n\n    total_rows = len(df)\n    null_counts = df.isnull().sum()\n    null_percentages = (null_counts / total_rows) * 100\n\n    result_df = pd.DataFrame({'Column': null_percentages.index, 'Null Percentage': null_percentages.values})\n\n    return result_df\n\noutput = null_percentage(input_df)\n\nprint(output)\n</code></pre> <pre><code>  Column  Null Percentage\n0     id             25.0\n1    dob              0.0\n2   name             50.0\n</code></pre> </p> Uniqueness <p><pre><code>import pandas as pd\n\ninput_data = [[1, 20240224, 'sample_name'], [None, 20240224, 'sample_name2'],[3, 20240224, None], [2, 20240224, None]]\ninput_schema = ['id', 'dob', 'name']\ninput_df = pd.DataFrame(input_data, columns = input_schema)\n\ndef uniqueness(df):\n'''\n    Function to calculate the uniqueness of values in each column of a pandas DataFrame.\n\n    Parameters:\n        df (DataFrame): Input DataFrame\n\n    Returns:\n        DataFrame: DataFrame with columns 'Column' and 'Uniqueness'\n    '''\n    unique_counts = df.nunique()\n    total_rows = len(df)\n    uniqueness_percentages = (unique_counts / total_rows) * 100\n\n    result_df = pd.DataFrame({'Column': uniqueness_percentages.index, '# of Unique Values': unique_counts})\n\n    return result_df\n\noutput = uniqueness(input_df)\n\nprint(output)\n</code></pre> <pre><code>     Column  # of Unique Values\nid       id                   3\ndob     dob                   1\nname   name                   2\n</code></pre> </p> Cardinality <p><pre><code>import pandas as pd\n\ninput_data = [[1, 20240224, 'sample_name'], [None, 20240224, 'sample_name2'],[3, 20240224, 'sample_name2'], [2, 20240224, None]]\ninput_schema = ['id', 'dob', 'name']\ninput_df = pd.DataFrame(input_data, columns = input_schema)\n\ndef cardinality(df, column_name):\n'''\n    Function to calculate find what values a variable can have and how many times they appear each.\n\n    Parameters:\n        df (DataFrame): Input DataFrame\n        column_name (str): name of the column that you want to know the cardinality of.\n\n    Returns:\n        DataFrame: DataFrame with columns 'Value' and 'Count'\n    '''\n    value_counts = df[column_name].value_counts().reset_index()\n\n    value_counts.columns = [column_name, 'count']\n\n    return value_counts\n\noutput = cardinality(input_df, 'name')\n\nprint(output)\n</code></pre> <pre><code>           name  count\n0  sample_name2      2\n1   sample_name      1\n</code></pre> </p> Basic Stats <p>Uniqueness<pre><code>import pandas as pd\n\ninput_data = [[1, 'john', 23, 51], [2, 'lucy', 18, 103],[3,'claire', 55, 87], [4, 'sam', 44, 70]]\ninput_schema = ['id', 'name','age', 'weight']\ninput_df = pd.DataFrame(input_data, columns = input_schema)\n\ndef basic_stats(df):\n'''\n    Function to calculate the basic stats of values in each numerical column of a pandas DataFrame.\n\n    Parameters:\n        df (DataFrame): Input DataFrame\n\n    Returns:\n        DataFrame: DataFrame with columns 'Column', 'Mean', 'Median', 'Mode', 'Standard Deviation', 'Minimum', 'Maximum'\n    '''\n    numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n\n    rows = []\n\n    for column in numerical_columns:\n\n        mean = df[column].mean()\n        median = df[column].median()\n        std = df[column].std()\n        min = df[column].min()\n        max = df[column].max()\n\n        rows.append([column, mean, median, std, min, max])\n\n    schema = ['column_name', 'mean', 'median', 'std', 'min', 'max']\n    stats = pd.DataFrame(rows, columns = schema)\n\n    return stats\n\n\n\noutput = basic_stats(input_df)\n\nprint(output)\n</code></pre> <pre><code>  column_name   mean  median        std  min  max\n0          id   2.50     2.5   1.290994    1    4\n1         age  35.00    33.5  17.454703   18   55\n2      weight  77.75    78.5  22.351361   51  103\n</code></pre> </p>"},{"location":"preparation/#assessment","title":"Assessment","text":"<p>Assessing the findings from data profiling means using that information to evaluate if the input data is fit for linkage purposes, and how the data profile affect the modelling choices for linkage.</p> <p>Data assessment</p> RecommendationsRAGWorked examples <p>This includes (but it is not limited to):</p> <ul> <li>Assessing if variables have inconsistent values across data sets, and how to address this issue</li> <li>Deciding which variables are most suitable to be used as blocking variables and for distance calculations, considering completeness, quality and validity</li> <li>Decide which variables to use if there are multiple variables carrying similar information (i.e. correlated)</li> </ul> <p>\ud83d\udd34 RED:  Data profiling results have not been assessed. There has been no effort to evaluate the input data for its suitability for linkage purposes. There is no consideration given to the consistency of variables across datasets, the selection of suitable blocking variables, or the handling of correlated variables.</p> <p>\ud83d\udfe1 AMBER:  Some level of data assessment has been conducted, but it is incomplete or inconsistent. There may be efforts to evaluate certain aspects of the data, such as identifying inconsistent variable values or selecting blocking variables, but these assessments are not comprehensive.</p> <p>\ud83d\udfe2 GREEN:  A rigorous assessment was carried out. Findings are documented and inform subsequent decisions on the linkage methods.</p> Profiling Assessment <p><pre><code>Table 1\n   gender  count\n0       1   2238\n1       2   1500\n2       9    387\n3       0    100\n\n\nTable 2\n   gender  count\n0       1   2555\n1       2   3002\n2       0    233\n</code></pre> In this case, table 1 that you are trying to link to table 2 has 4 possible values for gender, whereas table 2 only has 3. It is possible that this is a sign that \"Other\" genders are saved in several categories in one table and in less in the other table. When linking this may mean that you have to consider 0 or 9 in table 1 to be a match with 0 in table 2. </p> Blocking Rules Assessment <p><pre><code>       column Null percentage\n0  given_name             10%\n1      gender              1%\n2    postcode             30%\n</code></pre> In this case, using postcode as a blocking rule may not be appropriate, as due to such low data completeness in the column you might be blocking out records that are the correct match. Given name or gender may be more appropriate, though consideration of the low uniqueness of gender may make it less appropriate. </p> Correlated Values Assessment <p><pre><code>   Unique_ID            street postcode outcode   city\n0          1   Portland Street   LS31BL     LS3  Leeds\n1          2  Wellington Place   LS14AP     LS1  Leeds\n</code></pre> In this case, the different address columns are correlated, as they should all work together to get a consistent address. However, for the person with Unique_ID equal to 1, their street name does not align with their postcode, so one of them would be wrong. You need to consider whether you would use just one column for linking on address (e.g. just the postcode) or a hierarchy of the different columns, for example, if postcode does not match, but outcode and street do, how would you consider this? </p>"},{"location":"preparation/#enrichment","title":"Enrichment","text":"<p>Data enrichment is the process of enhancing the quality and usefulness of data for data linkage. It involves transforming, standardising, filtering, and cleaning the data to optimise its value.</p> <p>Data enrichment</p> RecommendationsRAGWorked examples <ul> <li>Transform: Create derived variables, recode categories, adjust formats, reshape data structures to ensure compatibility.</li> <li>Standardise: Convert formats (dates, text case), map variable categories, address recording inconsistencies by applying standardisation techniques.</li> <li>Exclude: Protect privacy and accuracy by removing sensitive data, national data opt-outs, and irrelevant fields.</li> <li>Clean: Identify and correct errors, missing values, inconsistencies, and outliers.</li> </ul> <p>Document the results of data transformations and standardisation to maintain transparency and reproducibility.</p> <p>\ud83d\udd34 RED:  There is no structured approach for data transformation. If transformations have been applied, they are scattered in the code, and have not been identified or documented. No efforts have been made to identify and exclude some sensitive, irrelevant or incorrect data.</p> <p>\ud83d\udfe1 AMBER:  While basic procedures for data transformation are in place, they aren't comprehensive. Some standardised practices can be observed, but partially documented. Efforts have been made to identify and exclude some sensitive, irrelevant or incorrect data, but the approach lacks comprehensiveness.</p> <p>\ud83d\udfe2 GREEN:  Data is systematically reviewed for the need of being transformed, standardised, filtered and cleansed. The processes are documented and identified in the code.</p> Transform gender values <p><pre><code>import pandas as pd\n\ngender_untransformed_schema = ['patient_id', 'gender']\n\nuntransformed_gender_data = [[1001, 1],[1002, 0],[1003, 2],[1004,1],[1005, 1],[1006, 9],[1007, 2],[1008,9]]\nuntransformed_gender_df = pd.DataFrame(untransformed_gender_data, columns = gender_untransformed_schema)\n\ndef transform_gender_formatting(df, column_to_standardise):\n'''\n    Converts encodings of 9 as 'unknown' for gender to 0\n    '''\n    df['transformed_gender'] = df[column_to_standardise].replace(9, 0)\n\n    return df\n\ntransformed_df = transform_gender_formatting(untransformed_gender_df, 'gender')\n\nprint(\"Untransformed gender\")\nprint(untransformed_gender_df)\nprint('\\n')\n\nprint(transformed_df)\n</code></pre> <pre><code>Untransformed gender\n   patient_id  gender  transformed_gender\n0        1001       1                   1\n1        1002       0                   0\n2        1003       2                   2\n3        1004       1                   1\n4        1005       1                   1\n5        1006       9                   0\n6        1007       2                   2\n7        1008       9                   0\n\n\n   patient_id  gender  transformed_gender\n0        1001       1                   1\n1        1002       0                   0\n2        1003       2                   2\n3        1004       1                   1\n4        1005       1                   1\n5        1006       9                   0\n6        1007       2                   2\n7        1008       9                   0\n</code></pre> </p> <p>In this example, a decision was made to standardise the gender encodings of '9' of unknown to '0' which codes to unspecified, of which there were some pre-existing values in the table. This standardisation could happen if there are multiple encodings for 'other' in one table, but only one mapping of 'other' in another.</p> Standardise text case <p><pre><code>import pandas as pd\n\nnames_unstandardised = ['patient_id', 'name']\n\nunstandardised_names_data = [[1001, 'alan'],[1002, 'BOB'],[1003, 'carla'],[1004,'David'],[1005, 'ERIC'],[1006, 'frances'],[1007, 'gReg'],[1008,'Hannah']]\nunstandardised_names_df = pd.DataFrame(unstandardised_names_data, columns = names_unstandardised)\n\ndef standardise_name_formatting(df, column_to_standardise):\n'''\n    Converts all text cases to upper case for names\n    '''\n    df['standardised_name'] = df[column_to_standardise].str.upper()\n\n    return df\n\nstandardised_df = standardise_name_formatting(unstandardised_names_df, 'name')\n\nprint(\"Unstandardised gender\")\n\nprint(unstandardised_names_df)\nprint('\\n')\n\nprint(standardised_df)\n</code></pre> <pre><code>Unstandardised gender\n   patient_id     name standardised_name\n0        1001     alan              ALAN\n1        1002      BOB               BOB\n2        1003    carla             CARLA\n3        1004    David             DAVID\n4        1005     ERIC              ERIC\n5        1006  frances           FRANCES\n6        1007     gReg              GREG\n7        1008   Hannah            HANNAH\n\n\n   patient_id     name standardised_name\n0        1001     alan              ALAN\n1        1002      BOB               BOB\n2        1003    carla             CARLA\n3        1004    David             DAVID\n4        1005     ERIC              ERIC\n5        1006  frances           FRANCES\n6        1007     gReg              GREG\n7        1008   Hannah            HANNAH\n</code></pre> In this example, standardising the text case makes linkage easier as variations in capitalisations are removed so 'ALAN', 'alan' and 'Alan' would correctly be treated as the same name. </p> Exclude records for linkage by patient consent <p><pre><code>import pandas as pd\n\npatient_consent = ['patient_id', 'consent']\n\nunfiltered_patient_data = [[1001, True],[1002, True],[1003, True],[1004,True],[1005, False],[1006, True],[1007, True],[1008,True]]\nunfiltered_consent_df = pd.DataFrame(unfiltered_patient_data, columns = patient_consent)\n\n\ndef filter_to_consenting_patients(df, patient_consent_column):\n'''\n    Removes any patients who have not consented to be contacted from the linkage\n    '''\n    return df[df[patient_consent_column]]\n\nconsenting_patients_df = filter_to_consenting_patients(unfiltered_consent_df, 'consent')\n\nprint(\"Unfiltered patient consent table\")\n\nprint(unfiltered_consent_df)\nprint('\\n')\n\nprint(\"Table of patients filtered to those who consent to their data being analysed\")\nprint(consenting_patients_df)\n</code></pre> <pre><code>Unfiltered patient consent table\n   patient_id  consent\n0        1001     True\n1        1002     True\n2        1003     True\n3        1004     True\n4        1005    False\n5        1006     True\n6        1007     True\n7        1008     True\n\n\nTable of patients filtered to those who consent to their data being analysed\n   patient_id  consent\n0        1001     True\n1        1002     True\n2        1003     True\n3        1004     True\n5        1006     True\n6        1007     True\n7        1008     True\n</code></pre> In this example, the data is filtered so any patients who do not consent for their data to be used are removed from the cohort to be linked. </p> Clean null name values <p><pre><code>import pandas as pd\n\nname_uncleaned_schema = ['patient_id', 'name']\n\nuncleaned_name_data = [[1001, 'alan'],[1002, 'BOB'],[1003, 'carla'],[1004,'David'],[1005, None],[1006, 'frances'],[1007, 'gReg'],[1008,None]]\nuncleaned_name_df = pd.DataFrame(uncleaned_name_data, columns = name_uncleaned_schema)\n\ndef clean_null_name_values(df, column_to_standardise):\n'''\n    Converts encodings of null to 'unspecified'\n    '''\n    df['cleaned_name'] = df[column_to_standardise].fillna('Unspecified')\n\n    return df\n\ncleaned_df = clean_null_name_values(uncleaned_name_df, 'name')\n\nprint(\"Uncleaned name table\")\nprint(uncleaned_name_df)\nprint('\\n')\n\nprint(\"Cleaned name table\")\nprint(cleaned_df)\n</code></pre> <pre><code>Uncleaned name table\n   patient_id     name cleaned_name\n0        1001     alan         alan\n1        1002      BOB          BOB\n2        1003    carla        carla\n3        1004    David        David\n4        1005     None  Unspecified\n5        1006  frances      frances\n6        1007     gReg         gReg\n7        1008     None  Unspecified\n\n\nCleaned name table\n   patient_id     name cleaned_name\n0        1001     alan         alan\n1        1002      BOB          BOB\n2        1003    carla        carla\n3        1004    David        David\n4        1005     None  Unspecified\n5        1006  frances      frances\n6        1007     gReg         gReg\n7        1008     None  Unspecified\n</code></pre> </p> <p>In this example, any null gender values are recoded to 'Unspecified'.</p> <p>If you have any ideas or feedback you'd like to give the team, feel free to contact us</p>"},{"location":"team-intro/","title":"Data Linkage Hub at NHS England","text":"<p>The Data Linkage Hub's work remit encompasses all things data linkage, from documenting the existing state of linkage in NHS England in the Person_ID handbook, to exploring better matching algorithms using probabilistic models and Splink, to creating a Quality Assurance Framework for Data Linkage.</p> <p>Data Linkage is a business-critical process within many government organisations, including NHS England. Being able to link patients across their care journey, making sure that under-represented populations are not lost in the cracks, and ensuring compatibility when using several data sets for the same purpose is a pillar of why the Data Linkage Enhancement team exists.</p>"},{"location":"team-intro/#data-linkage-enhancement-vision","title":"Data Linkage Enhancement Vision","text":"<p>The vision for the Data Linkage Hub is to become a key driver in enhancing patient health, directly or indirectly, by establishing a centre of excellence in quality and consistent data linkage practices.</p> <p>The hub aims to:</p> <ol> <li>Implement ethical and high-quality data linkage practices.</li> <li>Enforce these practices in collaboration with the broader data and analytics community in NHS England and beyond.</li> <li>Facilitate easier integration across departments.</li> <li>Develop comprehensive solutions that consider all stakeholders.</li> <li>Promote continuous improvement and adaptation to new or changing requirements.</li> </ol>"},{"location":"team-intro/#work-we-do","title":"Work we do","text":""},{"location":"team-intro/#quality-assurance-framework","title":"Quality Assurance Framework","text":"<p>If we want to achieve a consistent and high quality approach to linking data, which allows for robust, transparent and auditable results, we also need a framework to operate within. Hence, this workstream aims at creating, testing and implementing in the business process a Quality Assurance Framework for Data Linkage.</p>"},{"location":"team-intro/#better-matching-algorithm","title":"Better Matching Algorithm","text":"<p>We're currently working on implementing a probabilistic linkage model using Splink, in order to improve linkage outcomes, and by extension, patient outcomes.</p>"},{"location":"team-intro/#data-linkage-as-a-service","title":"Data Linkage as a service","text":"<p>This is the umbrella covering everything else we do. This stream of work encompasses:</p> <ul> <li>Identifying points of collaboration with other government departments</li> <li>Mapping the stakeholders involved in data linkage - both internal and external</li> <li>Feeding user needs into an overall Data Linkage vision</li> </ul> <p>To find out more about what the team is up to, visit our NHS England data science website.</p>"},{"location":"triage/","title":"Triage","text":""},{"location":"triage/#triage-purpose","title":"Triage purpose","text":"<p>This is an exercise which linkers will do alongside the data linkage enhancement team at the start of an engagement with the quality assurance framework for data linkage.  There are two objectives from this work:</p> <ul> <li>Firstly, it will start a conversation to identify potential linkage issues </li> <li>Secondly, this log forms an important part of the documentation alongside the framework</li> </ul> <p>It is worth noting that the questions have been formulated for NHS England use cases, and as such may not be applicable for all organisations. </p> <p>If you would like to keep track of your triage responses, download our triage log here.</p>"},{"location":"triage/#multiple-choice-triage-questions","title":"Multiple choice triage questions","text":"<p>The first section of questions are multiple choice to establish some of the project\u2019s core tenets, and you'd be expected to choose one of the options listed in the bullet points.</p> Dimension  Question Options Why is this question important? Engagement start What stage of your linkage project have you started engagement with the QAFDL? Start of the project This affects the order with which elements of the QAFDL should be considered Some project work has been completed, but work is ongoing Linkage has been completed, engaging with QAFDL for audit purposes Linkage project type What project type does this engagement fall under? Type 1 Categorising projects in this way can help with transfer of lessons learnt, and signposting the most relevant documentation Type 2 Type 3 Type 4 Types of linkage algorithm Which type of linkage algorithm are you using? If you are not sure, carry out the Preparation phase on the QAF log to discover the best option for your project. Then, come back to this question and select the most appropriate answer. Are you constrained in using a pre-determined linkage algorithm (e.g. MPS)? Simple join The data linker might need to use a specific, previously established methods for linking data, which affects how we check the quality of the linkage. Established linkage algorithms like MPS Established linkage algorithms + additional rules OR Ad-hoc models Commissioned to third-parties Purpose For what purpose are the data sets being linked? Direct care The purpose influences the quality assurance needed, for example, linked data used for direct patient care requires the highest quality. Depending on whether the linkage is defining a cohort or defining attributes you will want to have differing levels of sensitivity and recall. Non-direct care Risk What is the risk associated with data linkage inaccuracies? (missing a link making the wrong one) High risk  Understanding the risks involved allows us adjust our data linkage approach to ensure accuracy and reliability for its intended use. Moderate risk Low-risk  Profile of data content What is the importance profile of the data ? High profile health data The project's significance is determined by how errors could impact patient safety and may be influenced by other factors like business or politics. Normal profile Sensitivity of data  How sensitive is the data we are using? Sensitive data We adjust our quality checks based on how sensitive the data is to protect it properly during linkage. Non-sensitive data Frequency How often do you plan on linking these datasets? One-off The linkage frequency impacts how we approach the efficiency and repeatability of the process. Repeated (yearly, monthly) Repeated (weekly, daily) Live Expertise  How experienced is the person linking the data? Intermediate+ The skill level of the person linking the data affects quality; our framework guides less   experienced staff through the necessary steps. Novice"},{"location":"triage/#extended-triage-questions","title":"Extended triage questions","text":"<p>This second section asks more extended questions to ascertain a more in-depth view of the project\u2019s risk profile.</p> Additional questions Why is this question important? Related QAFDL aspect Entity being linked What entity is being linked (person, event, organisation,etc)? Knowing what we're linking\u2014people, events, or organisations\u2014guides our choice of linking variables and the reference master dataset used. Configuration of linkage parameters/settings Common characteristics used What common linkage fields do you have available in the two assets for linkage? (NHS numbers, MPS_ID, pseudo (NHS number or MPS_ID), DOB, etc To link data successfully, we need to identify fields that are sufficient to identify a match. Profiling Metadata Is metadata and documentation available for all datasets? Having common linking fields might not be enough for a successful linkage. Understanding them is important to define how they need to be adjusted or derived. Profiling Benchmarking What level of overlapping are you expecting from the data sets being linked? Linked datasets may represent different groups, so knowing how much overlap to expect helps us to predict acceptable linkage rates. Verification and validity Involvement of reference master data (a \"spine\") Do you need to link data sets to a reference master data set (a \"spine\") or between them or both? Linking to a main reference dataset, or \"spine,\" is best practice. It helps to connect different datasets and track changes over time, like when a patient moves house. Techniques and tools Synchronicity of data collection Do the data sets being linked refer to the same time frame? And are the PIDs being collected at the same time than the analytical measures of interest? If datasets span different times, we must adjust for these differences and consider how they might affect the linking process, potentially evaluating how errors in linkage change over time. Profiling, assessment Source of data What categories do the data sets being linked fall into? How was the data collected? Is it clinically validated, or patient inputted? (e.g. Cohort from external researchers, Core assets, DARS assets, Scraped data , Collected by NHSE, Sent by OGD) This context can provide lots of clues as to how best to link, what kind of quality we are expecting with respect to outputs, and what kind of quality assurance is needed post linkage. The source of data used in the linkage process can support decisions around other dimensions. For example, core assets are usually linked to reference master data sets via pipelines set up and maintained by the data engineering team. Data access Data types What data types are involved in the linkage (numerical, free text, binary, strings, etc)? The nature of the data we link (numbers, text, yes/no answers) affects how we prepare it for linking and which tools we choose. Data types need to be consistent across linkage variables between datasets. Profiling Data quality What is the expected data quality of the data sets being linked? (high, low) Data quality drives our choice of linking methods\u2014better quality data allows for simpler linking techniques. Assessment Number of data sets How many data sets will be linked? When linking several datasets, we must consider how they'll group together and how to resolve any mismatches, shaping our evaluation process. Configuration of linkage parameters/settings Environments where the linkage is executed Are you constrained in which environment will you be executing the linkage (e.g. DAE, DPS, UDAL, NDSR, RDS, FDP, etc)? The setting where we link data can dictate what data fields, tools, and methods we can use, possibly restricting our options of linkage algorithm. Computational resources, Safety, Configuration of linkage parameters/settings, Techniques and tools Environments where the assets reside Where do the assets reside (DAE, UDAL, RDS, etc)? Knowing the location of our data helps anticipate and plan for any hurdles, like secure transfers and privacy protection needs. Safety Customer request Is the customer asking for the highest level of quality assurance regardless of the triage outcomes? A project might be commissioned by a customer that requires the highest level of quality assurance regardless of the project characteristics.  The opposite may also occur, however, it is always recommended that the minimum suggested level of quality assurance is delivered to guarantee a professional outcome. Knowledge management, Quality of linkage Resources Which resources (and what type) do you have available to carry out the linkage? Our desired quality level might be limited by available resources, and we need to be clear when we can't meet our ideal standards. Uncertainty management"},{"location":"quality-assurance-framework/introduction/","title":"Introduction to the Quality Assurance Framework for Data Linkage (QAFDL)","text":""},{"location":"quality-assurance-framework/introduction/#purpose-of-qafdl","title":"Purpose of QAFDL","text":"<p>Data linkage is not a by-product of analysis, but a critical modelling step requiring an appropriate level of quality assurance. The objective of understanding and consolidating best practices in data linkage led to the creation of the Quality Assurance Framework.</p> <p>The Quality Assurance Framework for Data Linkage acts as a guide designed to provide a structured approach to managing and assuring the quality of data linkage processes in NHS England, but we welcome collaboration from other organisations. </p> <p>The QAFDL framework offers support for all stages of data linkage, from discovery to delivery. It is advised that the framework is consulted regularly to ensure that the quality assurance principles it contains are included in the project from inception. We view the QAFDL as a 'living' framework which we will iterate over time as it is implemented into our buisness processes.</p> <p>The broad aims are:</p> <ul> <li> <p>To equip and empower practitioners with the means to decide the appropriate levels of quality assurance.</p> </li> <li> <p>Function as a tracking instrument, enabling comprehensive evaluation of all dimensions described by the framework and facilitating the assessment of data linkage projects.</p> </li> </ul> <p>The QAFDL is based on three principles, linkage quality, transparency and safety. It can help boosting users' trust in the validity of the linked data for their analysis or research. It guides linkers and users on managing and interpreting linked data effectively, providing insights into the limitations of their linked data assets, enabling well-informed decisions and robust analyses. Moreover, the QAFDL promotes ethical and responsible use of linked data by suggesting best practices for data privacy and governance.</p>"},{"location":"quality-assurance-framework/introduction/#the-5ws-of-the-qafdl","title":"The 5W's of the QAFDL","text":"What Why Who When Where How Identifies key areas for assessment in data linkage projects. Provides tools for practitioners to determine quality assurance levels. Helps all involved understand complexities, recognize data strength, guide informed decisions. Primarily data linker's job, with recommended review by another person and reference by data users. Supports all stages, from discovery to delivery. Regular consultation advised. Essential in any environment where data linkage projects are planned and executed. Accessible to all involved for regular updates and review. Level of quality assurance varies by project, determined by data linker and users."},{"location":"quality-assurance-framework/introduction/#overview-of-the-quality-assurance-framework-dimensions","title":"Overview of the Quality Assurance Framework dimensions","text":"<p>Over 4 key dimensions, the QAFDL outlines best practices, principles, and strategies to facilitate high-quality data linkage. </p>"},{"location":"quality-assurance-framework/introduction/#definition-of-data-linkage","title":"Definition of data linkage","text":"<p>Data linkage refers to the process of connecting different datasets to create a more comprehensive view of an entity. An entity can be a patient, events, organisations, etc.</p> <p>Good reads</p> <ul> <li>Guidance document: Data Linkage for people who work with data in government by the Best Practice and Impact Division OSR</li> <li>Guidance document - What is data linkage, examples, challenges, methods for linkage, quality assessment and more: Joined up data in government: The future of data linking methods</li> </ul> <p>Data linkage is essential for healthcare research, statistics, and direct care, as it enables seamless integration of information and greatly impacting the quality of healthcare outcomes.</p>"},{"location":"quality-assurance-framework/introduction/#benefits-of-data-linkage","title":"Benefits of data linkage","text":"<p>The NHS, as stated on the Long Term Plan, aims to use data more effectively across health and social care services, for both individual care and for planning and research purposes and this can be done through data linkage. Other benefits are:</p> <ul> <li> <p>Enhanced Insights: By combining data from different data sources, more in-depth information can be learnt about specific people, groups of people, or occurrences. Linkage also helps policy makers in the development, and evaluation of policies and programmes.</p> </li> <li> <p>Longitudinal Analysis and research opportunities: Data linkage makes it possible to follow people over time, enabling longitudinal research that can more accurately highlight trends and patterns. Also, opens new possibilities for research that would not be feasible with isolated datasets.</p> </li> <li> <p>Risk Management: By spotting trends and correlations that might otherwise go unnoticed in isolated datasets, linked data can help organisations foresee and address possible problems.</p> </li> <li> <p>Reduced burden in providers: Data linkage reduces the need to collect the same information multiple times, thereby reducing the burden on providers. Data linkage can also reduce costs associated with duplicate efforts in data collection.</p> </li> <li> <p>Improved Data Quality: By facilitating the comparison of data across many sources, linkage enables the identification and correction of discrepancies or errors.</p> </li> <li> <p>Reducing Bias: When data from different sources is linked, it can help reduce selection bias in research studies, leading to more reliable findings.</p> </li> <li> <p>Service Delivery: In healthcare and social services, data linkage can improve service delivery by providing a more holistic view of a person's needs and interactions with different services.</p> </li> </ul> <p>Good reads</p> <ul> <li>Full report - Talks about the benefits of linking data sources for public health research, as well as the opportunities and challenges associated with doing so and how to get around them: Enabling Data Linkage to Maximise the Value of Public Health Research Data</li> </ul>"},{"location":"quality-assurance-framework/introduction/#useful-links","title":"Useful links","text":"<p>Joined up data in government: the future of data linking methods. This cross-government review contains contributed articles on state-of-the-art data linking methods and makes recommendations for government data linkage. </p> <p>Some more info and links available from here as well Data linkage \u2013 Government Analysis Function. </p> <p>A nice short read Developing standard tools for data linkage: February 2021 - Office for National Statistics. </p> <p>A more holistic report by the Office for Statistical Regulations (OSR) is the Data Sharing and Linkage for the Public Good: Follow-Up Report. </p> <p>Intro to data linkage is a data linkage course offered by the ONS Learning Hub.</p> <p>Ministry of Justice has a fantastic set of introductory material to data linkage, and especially probabilistic data linkage, as they are the authors of Splink, a Python package for probabilistic data linkage at scale. Particularly interesting the Bias in Data Linking blog post. </p> <p>Robin Linacre, one of the lead computer scientists behind Splink, authored this blog/introductory course to Probabilistic Linkage. </p> <p>Uni of Southampton also offers Introduction to Data Linkage. This is a full-immersion 2-day course offered in person or online by Prof. Katie Harron and Dr. James Doidge. </p> <p>To find out more about what the team is up to, visit our NHS England data science website.</p>"}]}